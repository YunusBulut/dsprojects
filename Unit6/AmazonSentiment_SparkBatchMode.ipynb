{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing/Predicting Sentiment From Amazon Reviews\n",
    "\n",
    "In this notebook, I use Spark (in batch mode only) to build a pipeline that can take in set of amazon reviews, featurize them using TFIDF vectors, and then use Logistic Regression to predict sentiment (where >3.5 stars is positive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id, create_map, lit\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, IDFModel, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some constants to initiate our spark context\n",
    "data = \"examples/AmznInstantVideo.json\"\n",
    "name = \"AMZN_Sentiment\"\n",
    "SPARK_URL = \"local[*]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a spark context, and then a sql context to load and manipulate the df/rdd\n",
    "sc = SparkSession.builder.appName(name).master(SPARK_URL).getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in the file, then examine:\n",
    "raw =sqlContext.read.json(data)\n",
    "raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+-----------+--------------+--------------------+--------------+\n",
      "|      asin|helpful|overall| reviewTime|    reviewerID|        reviewerName|unixReviewTime|\n",
      "+----------+-------+-------+-----------+--------------+--------------------+--------------+\n",
      "|B000H00VBQ| [0, 0]|    2.0| 05 3, 2014|A11N155CW1UV02|            AdrianaM|    1399075200|\n",
      "|B000H00VBQ| [0, 0]|    5.0| 09 3, 2012|A3BC8O2KCL29V2|             Carol T|    1346630400|\n",
      "|B000H00VBQ| [0, 1]|    1.0|10 16, 2013| A60D5HQFOTSOM|Daniel Cooper \"da...|    1381881600|\n",
      "|B000H00VBQ| [0, 0]|    4.0|10 30, 2013|A1RJPIGRSNX4PW|      J. Kaplan \"JJ\"|    1383091200|\n",
      "|B000H00VBQ| [1, 1]|    5.0|02 11, 2009|A16XRPF40679KG|       Michael Dobey|    1234310400|\n",
      "+----------+-------+-------+-----------+--------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw.select('asin','helpful','overall','reviewTime','reviewerID',\n",
    "           'reviewerName','unixReviewTime').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          reviewText|             summary|\n",
      "+--------------------+--------------------+\n",
      "|I had big expecta...|A little bit bori...|\n",
      "|I highly recommen...|Excellent Grown U...|\n",
      "|This one is a rea...|Way too boring fo...|\n",
      "|Mysteries are int...|Robson Green is m...|\n",
      "|This show always ...|Robson green and ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw.select('reviewText','summary').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37121\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#Examine the dimensions:\n",
    "print(raw.count())\n",
    "print(len(raw.columns)) #s/b 9 based on the above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts star ratings to a boolean sentiment score\n",
    "scoremapping = {\n",
    "    0.0: 0,\n",
    "    1.0: 0,\n",
    "    2.0: 0,\n",
    "    3.0: 0,\n",
    "    4.0: 1,\n",
    "    5.0: 1}\n",
    "\n",
    "mapping_expr = create_map([lit(x) for x in chain(*scoremapping.items())])\n",
    "\n",
    "raw = raw.withColumn('sentiment',mapping_expr[raw['overall']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets (30% held out for testing)\n",
    "(trainingData, testData) = raw.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          reviewText|               words|\n",
      "+--------------------+--------------------+\n",
      "|Mysteries are int...|[mysteries, are, ...|\n",
      "|This one is a rea...|[this, one, is, a...|\n",
      "|This show always ...|[this, show, alwa...|\n",
      "|I discovered this...|[i, discovered, t...|\n",
      "|It beats watching...|[it, beats, watch...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a tokenizer for the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\n",
    "\n",
    "#and transform the training data just to demonstrate it works\n",
    "wordsData = tokenizer.transform(trainingData)\n",
    "wordsData.select('reviewText', 'words').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|          reviewText|               words|         rawFeatures|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Mysteries are int...|[mysteries, are, ...|(100,[1,9,10,13,1...|\n",
      "|This one is a rea...|[this, one, is, a...|(100,[4,10,14,20,...|\n",
      "|This show always ...|[this, show, alwa...|(100,[0,3,6,7,9,1...|\n",
      "|I discovered this...|[i, discovered, t...|(100,[0,1,2,3,4,6...|\n",
      "|It beats watching...|[it, beats, watch...|(100,[7,20,26,29,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create simple term frequency transformer\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=100)\n",
    "\n",
    "#and transform the training data just to demonstrate it works\n",
    "hashedData = hashingTF.transform(wordsData)\n",
    "hashedData.select('reviewText','words','rawFeatures').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|rawFeatures                                                                                                                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(100,[1,9,10,13,15,18,20,22,30,33,38,52,61,68,72,81,83,90],[1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2.0,1.0,1.0])|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Examination of the output\n",
    "hashedData.select('rawFeatures').show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tfidfFeatures                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(100,[1,9,10,13,15,18,20,22,30,33,38,52,61,68,72,81,83,90],[1.4486081643747462,1.4548684813609989,0.25561063141378165,0.9040025269475227,1.0004374697646876,0.8130928802244595,1.0006473113350727,1.404906431304516,0.7199676630525657,0.43538024055159935,1.7246013362419772,0.9717916974190047,1.066908196317113,0.548509571174008,0.7894427867557378,0.8048442765780601,0.6801923406864785,1.5086786615400052])|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a transformer that will scale the term frequences by the inverse document frequency\n",
    "idf = IDF(minDocFreq=3, inputCol='rawFeatures', outputCol='tfidfFeatures').fit(hashedData)\n",
    "\n",
    "#and transform the training data just to demonstrate it works\n",
    "tfidf = idf.transform(hashedData)\n",
    "tfidf.select('tfidfFeatures').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|          reviewText|sentiment|       tfidfFeatures|\n",
      "+--------------------+---------+--------------------+\n",
      "|Mysteries are int...|        1|(100,[1,9,10,13,1...|\n",
      "|This one is a rea...|        0|(100,[4,10,14,20,...|\n",
      "|This show always ...|        1|(100,[0,3,6,7,9,1...|\n",
      "|I discovered this...|        1|(100,[0,1,2,3,4,6...|\n",
      "|It beats watching...|        0|(100,[7,20,26,29,...|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Recap of raw input, labels, and features:\n",
    "tfidf.select('reviewText','sentiment','tfidfFeatures').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a label indexer \n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"indexedLabel\").fit(tfidf)\n",
    "\n",
    "# Fit a feature indexer\n",
    "featureIndexer = VectorIndexer(inputCol=\"tfidfFeatures\", outputCol=\"indexedFeatures\", maxCategories=4).fit(tfidf)\n",
    "\n",
    "# Train a Logistic Regression\n",
    "lr = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel', predictionCol='pred')\n",
    "\n",
    "# Chain feature transformers, indexers and regression in a Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, \n",
    "                            labelIndexer, featureIndexer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+--------------------+\n",
      "|          reviewText|sentiment|pred|         probability|\n",
      "+--------------------+---------+----+--------------------+\n",
      "|I had big expecta...|        0| 0.0|[0.81175358172645...|\n",
      "|I highly recommen...|        1| 0.0|[0.87999870113215...|\n",
      "|This is the best ...|        1| 0.0|[0.78954232320191...|\n",
      "|if this had to do...|        0| 0.0|[0.83113051584153...|\n",
      "|Watched it for Ke...|        1| 0.0|[0.80906059339862...|\n",
      "+--------------------+---------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit the full model to the training data\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "#Predict test data \n",
    "predictions = model.transform(testData)\n",
    "predictions.select('reviewText', 'sentiment','pred','probability').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.798482\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"pred\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionAndLabels = predictions.select('pred','sentiment').rdd\n",
    "type(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'serializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-436a79c9c16d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictionAndLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/mllib/evaluation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scoreAndLabels)\u001b[0m\n\u001b[1;32m     50\u001b[0m         df = sql_ctx.createDataFrame(scoreAndLabels, schema=StructType([\n\u001b[1;32m     51\u001b[0m             \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             StructField(\"label\", DoubleType(), nullable=False)]))\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mjava_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBinaryClassificationMetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjava_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmapPartitionsWithIndex\u001b[0;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPipelinedRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmapPartitionsWithSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prev, func, preservesPartitioning)\u001b[0m\n\u001b[1;32m   2433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bypass_serializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreservesPartitioning\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'serializer'"
     ]
    }
   ],
   "source": [
    "metrics = BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

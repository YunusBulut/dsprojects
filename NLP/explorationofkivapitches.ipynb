{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"1. Data cleaning / processing / language parsing\n1. Create features using two different NLP methods: For example, BoW vs tf-idf.\n1. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n1. Assess your models using cross-validation and determine whether one model performed better.\n1. Pick one of the models and try to increase accuracy by at least 5 percentage points."},{"metadata":{"trusted":true,"_uuid":"3a755e87cf2f14d2c125d2811e096d9ac0da735d"},"cell_type":"code","source":"raw = pd.read_csv('../input/kiva_loans.csv')\nraw.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d3721af947788541bc31893d87e96e278384756"},"cell_type":"code","source":"#Goal: examine language differences between genders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e39e868a0ccf2a57f8a376853a17b15afc0c9c94"},"cell_type":"code","source":"#Let's examine the possibilities\nraw.groupby('borrower_genders')['use'].agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa33ecf49905fa2221e938a3cfb7b50ba91cffed"},"cell_type":"code","source":"#What a mess. Let's consolidate:\nraw['borrower_genders'] = raw['borrower_genders'].astype('str') \nfeat = []\nfor i in raw.iterrows():\n    val = i[1][17]\n    if ('female' in val) & (', male' in val):\n        feat.append('both')\n    elif 'female' in val:\n        feat.append('female')\n    elif ('female' not in val) & ('male' in val):\n        feat.append('male')\n    else:\n        feat.append(val)\nraw['borr_gender'] = feat\nraw.groupby('borr_gender')['use'].agg('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0077d2dc5678980cb66bd54e218e0deaaf3e6ad"},"cell_type":"code","source":"df = raw[['id', 'use', 'borr_gender']]\ndel raw\ndf = df[(df.borr_gender == 'female') | (df.borr_gender == 'male')]\n#Reduce size\ndf = df.sample(20000)\nprint(df.shape)\ndf['borr_gender'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24dd6a5b9d53de0097c5c182f0848a6e67f2ccf9"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99844a64b1dc9ffe0891c09a53456a9e04445bd0"},"cell_type":"markdown","source":"# BOW Feature Generation"},{"metadata":{"trusted":true,"_uuid":"071be3380bb822cc56ebb675eb8fd3ea844654e9"},"cell_type":"code","source":"import scipy\nimport sklearn\nimport spacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5591ee9b48ea118fcc26d97120170526690b558"},"cell_type":"code","source":"nlp = spacy.load('en', parser=False, entity=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"240a229a7a5afa2a00aa0cdb144540bb7fe830ec"},"cell_type":"code","source":"df['tokens'] = df['use'].apply(lambda y: nlp(y))\ndf = df.reset_index()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a105b4a04d41c24e9bec79fc182c7f8589e9479"},"cell_type":"code","source":"# Utility function to create a list of the 2000 most common words.\ndef bag_of_words(text, n):\n    allwords=[]\n    for _ in text:    \n        # Filter out punctuation and stop words.\n        allwords.append([token.lemma_\n                    for token in _\n                    if not token.is_punct\n                    and not token.is_stop])\n    allwords = [item for sublist in allwords for item in sublist]    \n    # Return the most common words.\n    return [item[0] for item in Counter(allwords).most_common(n)]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fee1e20ef194e2815c3037f62879ac0f089fe66"},"cell_type":"code","source":"twoKwords = bag_of_words(df.tokens, 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0313706ae937b91c836f007247df6f65b3aad4b7"},"cell_type":"code","source":"list(twoKwords)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6a2cd82ddb2be07e0a1d9c6deee7bb5544eb624"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1fb89225aad7a9884d45433a702a1728486d4cf"},"cell_type":"code","source":"vectorizer = CountVectorizer(vocabulary=twoKwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc1aab317c9d1e962468ae2c66c91d3309b2dec9"},"cell_type":"code","source":"X = vectorizer.fit_transform(df['use'])\ndf2 =  pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\nword_counts = pd.concat([df,df2], axis=1)\nprint(word_counts.shape)\ndel df2\nword_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60cb6044d7de21e62c779b835adb241094958903"},"cell_type":"markdown","source":"# Try Random Forest"},{"metadata":{"trusted":true,"_uuid":"ba5af4381bc0c542d24cf8f6e9665cdaf38c22c9"},"cell_type":"code","source":"from sklearn import ensemble\nfrom sklearn.model_selection import train_test_split, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"968c94746ae36811a9b4e612a349ea445772581d"},"cell_type":"code","source":"rfc = ensemble.RandomForestClassifier(n_estimators=25)\nY = word_counts['borr_gender']\nX = np.array(word_counts.drop(['index','id','use','borr_gender','tokens'], 1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y,\n                                                    test_size=0.4,\n                                                    random_state=0)\ntrain = rfc.fit(X_train, y_train)\n\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(rfc,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88226ba14b76c9dc2323e7c4e958027e8c367aaf"},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"a80dce452ccc30892f9a0443548dea3c4ddfdaed"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver='liblinear')\ntrain = lr.fit(X_train, y_train)\nprint(X_train.shape, y_train.shape)\nprint('Training set score:', lr.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(lr,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4977d2e633ed3dec129e092c4c2535fa583f7de"},"cell_type":"markdown","source":"Logistic Regression appears to be doing better. \nI will not try to improve rfc performance by 5%"},{"metadata":{"trusted":true,"_uuid":"ced51e743ff7ec64fa63a6205359c4bd87af5bae"},"cell_type":"code","source":"rfc = ensemble.RandomForestClassifier(n_estimators=25,\n                                     max_depth=100)\nY = word_counts['borr_gender']\nX = np.array(word_counts.drop(['index','id','use','borr_gender','tokens'], 1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y,\n                                                    test_size=0.4,\n                                                    random_state=0)\ntrain = rfc.fit(X_train, y_train)\n\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(rfc,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1feaa036a7c5038354546fb166e1d47419da34a"},"cell_type":"code","source":"rfc = ensemble.RandomForestClassifier(n_estimators=25,\n                                     max_depth=20)\nY = word_counts['borr_gender']\nX = np.array(word_counts.drop(['index','id','use','borr_gender','tokens'], 1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y,\n                                                    test_size=0.4,\n                                                    random_state=0)\ntrain = rfc.fit(X_train, y_train)\n\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(rfc,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da76fe737470cdaf4038228a4abf4919f2a39638"},"cell_type":"code","source":"#That's not helping, let's try including more features...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a31293ff61229fc5fe688acbd7fdb59e3d9fe67e"},"cell_type":"code","source":"threeKwords = bag_of_words(df.tokens, 3000)\nvectorizer = CountVectorizer(vocabulary=threeKwords)\n\nX = vectorizer.fit_transform(df['use'])\ndf2 =  pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\nword_counts = pd.concat([df,df2], axis=1)\nprint(word_counts.shape)\ndel df2\nword_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62df797ac0fa12de31c23facb11300e96c8c6c81"},"cell_type":"code","source":"rfc = ensemble.RandomForestClassifier(n_estimators=25,\n                                     max_depth=100)\nY = word_counts['borr_gender']\nX = np.array(word_counts.drop(['index','id','use','borr_gender','tokens'], 1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    Y,\n                                                    test_size=0.4,\n                                                    random_state=0)\ntrain = rfc.fit(X_train, y_train)\n\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(rfc,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"207b545390f8d62af355dec18d28f1e970fd7a49"},"cell_type":"code","source":"rfc = ensemble.RandomForestClassifier(n_estimators=200,\n                                     max_depth=100)\ntrain = rfc.fit(X_train, y_train)\n\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(rfc,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdfd5673ac893f3830d25b5a72be604b9916a593"},"cell_type":"code","source":"#One more performance intensive try...\nrfc = ensemble.RandomForestClassifier(n_estimators=250,\n                                     max_depth=None)\ntrain = rfc.fit(X_train, y_train)\nprint('Training set score:', rfc.score(X_train, y_train))\nprint('\\nTest set score:', cross_val_score(rfc,X_test, y_test, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71644ba6c5a40f3c9ef618034ebfc5bdc2719e71"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4da1d002dcd66c9fd865d617319566e11d41d746"},"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than .X of the paragraphs\n                             min_df=4, # only use words that appear at least n times\n                             stop_words='english', \n                             lowercase=True, #convert everything to lower case\n                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n                            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bff6d358387bf119f99beab7c6831803832e6245"},"cell_type":"code","source":"use_tfidf=vectorizer.fit_transform(df['use'])\nprint(\"Number of features: %d\" % use_tfidf.get_shape()[1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"745ec790bc7ff9467202076fa65390e29b3add46"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22723ff7f948a0af6c208a23edad531affebda3f"},"cell_type":"code","source":"#Our SVD data reducer.  We are going to reduce the feature space\nsvd= TruncatedSVD(200)\nlsa = make_pipeline(svd, Normalizer(copy=False))\n# Run SVD on the training data, then project the training data.\nX_lsa = lsa.fit_transform(use_tfidf)\n\nvariance_explained=svd.explained_variance_ratio_\ntotal_variance = variance_explained.sum()\nprint(\"Percent variance captured by all components:\",total_variance*100)\n\n#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\nparas_by_component=pd.DataFrame(X_lsa,index=df.use)\nfor i in range(5):\n    print('Component {}:'.format(i))\n    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57a735c33ae6592e5f10521dd856bc24cec023f4"},"cell_type":"code","source":"# Random Forest attempt\nrfc = ensemble.RandomForestClassifier(n_estimators=25,\n                                     max_depth=None)\nY = df['borr_gender']\nX = X_lsa\n\ntrain = rfc.fit(X, Y)\nprint('\\nTest set score:', cross_val_score(rfc,X, Y, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82df187b598564792099d70167ba6abf2d6ca3b9"},"cell_type":"code","source":"# Random Forest attempt\nrfc = ensemble.RandomForestClassifier(n_estimators=250,\n                                     max_depth=100)\nY = df['borr_gender']\nX = X_lsa\n\ntrain = rfc.fit(X, Y)\nprint('\\nTest set score:', cross_val_score(rfc,X, Y, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"171b41da996f2ae4241c63969d7082dcb0def121"},"cell_type":"code","source":"lr = LogisticRegression(solver='liblinear')\ntrain = lr.fit(X, Y)\nprint('\\nCV score:', cross_val_score(lr,X, Y, cv=3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"583d91cce0675223216383fed2350a3d5ca4fdad"},"cell_type":"markdown","source":"Both of these methods seem to be convergin on .8 in cross validation. "},{"metadata":{"trusted":true,"_uuid":"d677c12ee0e695e51d8fe592a7569fbd6d597619"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}